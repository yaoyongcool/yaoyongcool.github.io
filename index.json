[{"categories":null,"content":"First_post ","date":"2025-10-07","objectID":"/first_post/:0:1","tags":null,"title":"First_post","uri":"/first_post/"},{"categories":null,"content":"LLaVA-mini LLaVA-mini: efficient image and video large multimodal models with one vision token 主要内容： 提出了一个基于查询的压缩方法和模态预融合。具体来说就是将视觉Token进行压缩，最少可以达到1个。但是视觉Token在前几layer比较重要，所以进行了模态预融合，不要降低精度。 Conclusion ： 学习一下这个思路，同时测评的时候有用到好几个数据集和评测方法，学习一下。 USERRL USERRL: TRAINING INTERACTIVE USER-CENTRIC AGENT VIA REINFORCEMENT LEARNING 主要内容： UserRL框架是一个“AI助手的训练营”，它主要做三件事： 提供训练场： 通过8个不同的Gym环境，让AI能系统地练习各种与人交互的技能。 设计评分规则： 提供了一套灵活的工具，让研究者可以实验不同的打分方式（比如上面说的回合级和轨迹级），找到最能训练出“聪明”AI的方法。 模拟真实用户： 用另一个AI（如Qwen3-32B或GPT-4o）来扮演“用户”，与训练的AI进行对话，从而产生大量、多样的互动数据。 Conclusion： 在使用RL的时候采用SFT很重要！训练AI助手就像教新人，得先进行“岗前培训”（SFT冷启动），教它最基本的对话规则，然后再用强化学习进行“实战演练”，这样才能越练越好。 要有大局观： 在训练时，鼓励AI关注“整段对话是否成功”（轨迹级评分），比纠结于“每句话说得对不对”（回合级区分）更重要。 陪练的水平很重要，但不是绝对的： 用一个很强的AI（如GPT-4o）来模拟用户，训练效果最好。但用便宜些的开源AI（如Qwen3-32B）当陪练，训练出的AI助手表现也不错，性价比高。 iRe-VLA Improving Vision-Language-Action Model with Online Reinforcement Learning 主要内容： 第0阶段：基础教学（监督学习） 目标：用专家数据训练一个初始的VLA模型。 方法：就像老师手把手教学生，让模型学习如何根据图像和指令做出正确的动作。 第1阶段：自主探索（强化学习 + 冻结主干） 目标：在环境中尝试新任务，探索新解法。 关键创新：冻结庞大的VLM主干网络，只训练轻量级的“动作头”。 好比：一个学生已经有了扎实的知识基础（VLM），现在让他去自由探索，但为了防止他学歪，我们只允许他微调自己的“动手能力”（动作头）。 好处： 稳定性：保护了模型的核心知识，防止训崩。 高效性：计算量大大降低，一张消费级显卡（如4090）就能跑。 第2阶段：巩固复习（监督学习） 目标：将第1阶段探索成功的经验，与原始专家数据融合，巩固所学。 方法：用所有成功的数据（包括专家数据和RL探索得到的新数据）再次训练整个模型（包括VLM主干）。 好比：学生把探索中发现的成功方法和老师教的标准答案放在一起复习，从而既掌握了新技巧，又没忘记旧知识，还加深了理解。 循环迭代：重复第1和第2阶段，模型的能力就像滚雪球一样，越来越强。 conclusion： 提出了新的 iRe-VLA 方法，该方法在在线强化学习阶段和监督学习阶段之间进行迭代 传统的RL方法（如PPO）直接微调整个模型会导致性能崩溃，而iRe-VLA方法训练非常稳定。 字节Seed1.5-VL 学习笔记参考多模态文件夹 Conclusion： 动态分辨率处理能力（基于2D RoPE位置编码） Seed-ViT和MiCo框架 混合强化学习 ，训练方法结合了RLHF和RLVR Visual-RFT Visual-RFT: Visual Reinforcement Fine-Tuning 主要内容： 将RFT的成功经验从数学、代码等纯文本领域，扩展到视觉多模态任务中，让视觉-语言大模型也能通过强化学习 Conclusion ： 可验证奖励函数、为视觉任务设计可验证奖励（检测任务和 分类任务） ","date":"2025-10-07","objectID":"/9.28-10.4%E6%97%A5/:0:0","tags":null,"title":"阅读笔记","uri":"/9.28-10.4%E6%97%A5/"}]